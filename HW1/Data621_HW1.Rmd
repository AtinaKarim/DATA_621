---
title: "Data621 - HW1"
author: "Devin Teran, Atina Karim, Tom Hill, Amit Kapoor"
date: "2/26/2021"
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r include=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(DataExplorer)
library(GGally)
library(ggplot2)
library(readr)
library(reshape2)
library(purrr)
library(tidyr)
library(corrplot)
library(MASS)
library(caret)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Overview
The data set contains approximately 2276 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.Below is a short description of the variables

* INDEX 				    - Identification Variable
* TARGET_WINS       - Number of wins
* TEAM_BATTING_H		- Base Hits by batters (1B,2B,3B,HR)
* TEAM_BATTING_2B		- Doubles by batters (2B)
* TEAM_BATTING_3B		- Triples by batters (3B)
* TEAM_BATTING_HR		- Homeruns by batters (4B)
* TEAM_BATTING_BB		- Walks by batters 
* TEAM_BATTING_HBP	- Batters hit by pitch (get a free base) 
* TEAM_BATTING_SO		- Strikeouts by batters 
* TEAM_BASERUN_SB		- Stolen bases 
* TEAM_BASERUN_CS		- Caught stealing 
* TEAM_FIELDING_E		- Errors 
* TEAM_FIELDING_DP	- Double Plays 
* TEAM_PITCHING_BB	- Walks allowed 
* TEAM_PITCHING_H		- Hits allowed 
* TEAM_PITCHING_HR	- Homeruns allowed 
* TEAM_PITCHING_SO	- Strikeouts by pitchers 

## Objective
To build a multiple linear regression model on the training data to predict *TARGET_WINS*, which is the number of wins for the team.



## Data Exploration

```{r read-csv}
# read data
baseball_df <- read.csv('https://raw.githubusercontent.com/hillt5/DATA_621/master/HW1/moneyball-training-data.csv')
baseball_eval <- read.csv('https://raw.githubusercontent.com/hillt5/DATA_621/master/HW1/moneyball-evaluation-data.csv')

head(baseball_df)
dim(baseball_df)
```




```{r, baseball-summary}
summary(baseball_df)
print('Number of observations:') 
nrow(baseball_df)
print('Observations per year, 1871 - 2006:') 
round(nrow(baseball_df)/(2006-1871),2)
```

Some columns have maximum values that are clearly outliers, like TEAM_PITCHING_H AND TEAM_PITCHING_HR. The assignment mentions that some of the season records were adjusted to match the performance during a 162-game season. There are 2276 seasons in the training set. Observations span 128 years, with an average of 17 teams playing per year. Based on a quick Google search, there were initially 8 teams in the MLB, and 30 teams in 2006. The MLB has two leagues of the same size since 1901, with the league sizes increasing in the late 20th century.  


```{r}
# distribution
plot_histogram(baseball_df)
```

```{r}
# against the response variable
plot_scatterplot(baseball_df, by = "TARGET_WINS")
```


```{r}
# boxplot for train dataset
plot_boxplot(baseball_df, by="TARGET_WINS")
```



## Feature Boxplots and Histograms

```{r, baseball-graphs}


#baseball_df %>%
#  keep(is.numeric) %>%
#  gather() %>% 
#  ggplot(aes(value)) +
#    facet_wrap(~ key, scales = "free") +
#    geom_boxplot()


#baseball_df %>%
#  keep(is.numeric) %>%
#  gather() %>% 
#  ggplot(aes(value)) +
#    facet_wrap(~ key, scales = "free") +
#    geom_histogram()

```

Based on this quick look of boxplots and histograms, there are several variables with skewed distributions that would benefit from transformation. Additonally, there are a few variables with bimodal distributions.



```{r, corrplot}

corrplot(cor(baseball_df[,2:17], use = 'complete.obs'))


```

Looking at the correlation plot, there appear to be several strong correlations between explanatory variables and the target.  
  
From an initial inspection, it appears the team should focus on getting players on base through hits or walks.  Contrary to what I would expect, teams can still win if the pitchers allow homeruns, hits and walks to the other team.
*Variables with Highest Positive Correlation with TARGET_WINS:*  
  * TEAM_BATTING_H = 0.47
  * TEAM_BATTING_HR = 0.42
  * TEAM_BATTING_BB = 0.47
  * TEAM_PITCHING_H = 0.47 
  * TEAM_PITCHING_HR = 0.42
  * TEAM_PITCHING_BB = 0.47

To win more games it makes sense the team will need to make fewer errors.  
*Variables with Strongly Negative Correlation with TARGET_WINS:*  
  
There were several batting variables which were related.  
*Positive Correlations between variables*:     
  * TEAM_PITCHING_H and TEAM_BATTING_H = 0.99  
  * TEAM_PITCHING_HR and TEAM_BATTING_HR = 0.99  
  * TEAM_PITCHING_BB and TEAM_BATTING_BB = 0.99    
  * TEAM_PITCHING_SO and TEAM_BATTING_SO = 0.99   
  
The pitchers who have more strikeouts allow fewer hits, which makes sense.  It's interesting that pitchers who have fewer strikeouts have fewer team batting hits.  Potentially due to the game being over in fewer innings and lower score games.  This would be an interesting point to look into more.  
*Negative Correlations between variables*:    
  * TEAM_PITCHING_SO and TEAM_BATTING_H  = -0.34  
  * TEAM_PITCHING_SO and TEAM_PITCHING_H = -0.34  

## Missing values

```{r, missing-values}


round(100*colSums(is.na(baseball_df))/nrow(baseball_df),2)


```

In terms of missing values, there are two variables missing many obserations. TEAM_BATTING_HBP is missing over 90% of its values, while TEAM_BASERUN_CS is missing just around 30%. Since so many observations are missing, imputing values could change the distributions considerably. To retain as many features as possible, I think it makes sense to explore these two variables first. The other affected missing values only have 5-10% misisng values. None of these appear to be stand-ins for 'zero' values, so mean values can be used insead.


I'll start by looking at TEAM_BATTING_HBP.


```{r, separate-hbp-df}

baseball_no_hbp <- baseball_df %>%    
  filter(is.na(TEAM_BATTING_HBP)) %>% #missing values for hbp
  dplyr::select(-TEAM_BATTING_HBP)  ## select all rows except hbp



baseball_hbp <- baseball_df %>%
  filter(!is.na(TEAM_BATTING_HBP)) #not missing values for hbp

```

I separated training data into two smaller dataframes, one with complete values for HBP and one omitting this variable.


```{r, cor-hbp-only}

corrplot(cor(baseball_df[,2:17], use = 'complete.obs')) 

corrplot(cor(baseball_hbp[,2:17], use = 'complete.obs'))

```

When HBP has values, it appears that there are no major changes in correlations.

```{r, cor-no-hbp}

corrplot(cor(baseball_df[,-c(1,11)], use = 'complete.obs'))  #all rows

corrplot(cor(baseball_no_hbp[,2:16], use = 'complete.obs')) #only rows  missing values for hbp

corrplot(cor(baseball_hbp[,-c(1,11)], use = 'complete.obs')) #only rows with values for hbp 

```


There are three new correlaton plots being considered: the first is all datapoints, then a plot with missing hbp values, and finally a plot for rows with hbp values same as the previous pair.  There appear to be no major discrepancies between missing values and the overall set. However, comparing missing values to available values does illustrate there are some distinct changes correlation when the hbp was recorded. This may be because HBP only represents only a small proportion of entries and has more variation. However, there also appear to be stronger correlations when HBP is added, which may help predict wins better than omitting altogether.


```{r, hbp-lm}

hbp_lm <- lm(baseball_hbp, formula = TARGET_WINS ~.-INDEX-TEAM_BATTING_HBP-TEAM_BATTING_SO-TEAM_BATTING_HR-TEAM_BASERUN_CS-TEAM_BATTING_H-TEAM_BASERUN_SB-TEAM_PITCHING_BB-TEAM_BATTING_2B-TEAM_BATTING_3B)


summary(hbp_lm)

baseball_hbp_dummy <- baseball_df %>%
  mutate(TEAM_HBP_YES_NO = case_when(!is.na(TEAM_BATTING_HBP) ~ 1, is.na(TEAM_BATTING_HBP) ~ 0)) %>%
  dplyr::select(-TEAM_BATTING_HBP)


summary(lm(baseball_hbp_dummy, formula = TARGET_WINS ~.-INDEX-TEAM_PITCHING_BB-TEAM_PITCHING_HR-TEAM_BATTING_H))


```

I compared two preliminary linear models that I arrived at through backward selection. Looking only at HBP-containing observations, there's a small increase in r-squared compared to a model that uses a dummy variable to consider whether values were available.


Next, I'll look at TEAM_BASERUN_CS, which was missing about 30% of its values.


```{r, cs-missing-values}


sum(baseball_df$TEAM_BASERUN_CS == 0, na.rm = TRUE)


hist(baseball_df$TEAM_BASERUN_CS, breaks = 30)


baseball_no_cs <- baseball_df %>%    
  filter(is.na(TEAM_BASERUN_CS)) %>% #missing values for hbp
  dplyr::select(-TEAM_BASERUN_CS)  ## select all rows except hbp



baseball_cs <- baseball_df %>%
  filter(!is.na(TEAM_BASERUN_CS)) #not missing values for hbp

```

Same as HBP, it appears CS did not miscode values of 0 as NA. I'm going to separate the dataset in the same way as HBP to see if there are differences in its distribution and correlation plots.

```{r, cs-distributions}


baseball_df %>% ##original histograms
  dplyr::select(-TEAM_BASERUN_CS) %>%
  keep(is.numeric) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()



baseball_cs %>% ##historgrams with seasons having CS statistics
  dplyr::select(-TEAM_BASERUN_CS) %>%
  keep(is.numeric) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()




baseball_no_cs %>% #histograms missing CS statistics
  keep(is.numeric) %>%
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()


```

After subsetting for availability of CS statistics, an interesting pattern emerges: our three bimodal variables, TEAM_PITCHING_HR, TEAM_BATTING_SO, and TEAM_BATTING_HR, are no longer bimodal.






```{r, compare-pitching-hr}

baseball_hbp_dummy <- baseball_hbp_dummy %>%
  mutate(TEAM_CS_YES_NO = case_when(!is.na(TEAM_BASERUN_CS) ~ '1', is.na(TEAM_BASERUN_CS) ~ '0')) 

ggplot(baseball_hbp_dummy, aes(x = TEAM_PITCHING_HR, fill = TEAM_CS_YES_NO)) +
  geom_histogram() +
  theme(legend.position = 'none')

ggplot(baseball_df, aes(x = TEAM_PITCHING_HR)) +
  geom_histogram()

```

```{r, compare-batting-hr}


ggplot(baseball_hbp_dummy, aes(x = TEAM_BATTING_HR, fill = TEAM_CS_YES_NO)) +
  geom_histogram() +
  theme(legend.position = 'none')

ggplot(baseball_df, aes(x = TEAM_BATTING_HR)) +
  geom_histogram()

```

```{r, compare-batting-so}


ggplot(baseball_hbp_dummy, aes(x = TEAM_BATTING_SO, fill = TEAM_CS_YES_NO)) +
  geom_histogram() +
  theme(legend.position = 'none')

ggplot(baseball_df, aes(x = TEAM_BATTING_SO)) +
  geom_histogram()

```

As these three histograms illustrate, the bimodal distributions are explained by missing CS values or not. Missing values explain both modes present in the overall histogram.



```{r, missing-values-visualization}
# Devin - start- baseball_df_fix not defined yet - did you want to move to after? I added the code here just so it wouldn't error out
baseball_df_fix <- baseball_df %>%
  mutate(TEAM_CS_YES_NO = case_when(!is.na(TEAM_BASERUN_CS) ~ 1, is.na(TEAM_BASERUN_CS) ~ 0)) %>% 
  mutate(TEAM_HBP_YES_NO = case_when(!is.na(TEAM_BATTING_HBP) ~ 1, is.na(TEAM_BATTING_HBP) ~ 0)) %>%
  dplyr::select(-c(TEAM_BATTING_HBP, INDEX, TEAM_BASERUN_CS))
# Devin - end- baseball_df_fix not defined yet - did you want to move to after? I added the code here just so it wouldn't error out

#Devin - still erroring out?
#image(is.na(baseball_df_fix),axes=FALSE,col=gray(1:0))
#axis(2, at=0:17/17, labels=colnames(baseball_df_fix))
#axis(1, at=0:2275/2275, labels=row.names(baseball_df_fix),las=2)

```

To better visualize the missing values, it looks like two of them overlap perfectly.


```{r, overlap-na}
baseball_df_fix[rowSums(is.na(baseball_df_fix)) > 0,] %>%
  dplyr::select(TEAM_PITCHING_SO, TEAM_FIELDING_DP, TEAM_BATTING_SO, TEAM_BASERUN_SB)




summary( lm(baseball_df_fix, formula = TEAM_BATTING_SO ~.-TARGET_WINS))
summary( lm(baseball_df_fix, formula = TEAM_PITCHING_SO ~.-TARGET_WINS))

```

It appears that TEAM_PITCHING_SO and TEAM_BATTING_SO are missing all of the same rows. By quickly running a linear model for either column shows that it's possible to approximate values from other season records.


```{R, zero-values}

baseball_df%>%
  dplyr::filter(TEAM_PITCHING_SO < 5)

baseball_df%>%
  dplyr::filter(TEAM_BATTING_SO < 5)


```
Lookin closer at these two variables, there are also some values that may be omitted if they are implausibly small. Going a whole season with zero strikeouts, pitching or batting, seems unlikely. It may make sense to recode these as NA and impute values onto them.


```{r, overlap-sb-cs}

baseball_df %>%
  dplyr::select(TEAM_BASERUN_CS, TEAM_BASERUN_SB) %>%
  filter(is.na(TEAM_BASERUN_SB))


hist(baseball_df$TEAM_BASERUN_SB)

hist(log(baseball_df$TEAM_BASERUN_SB))

baseball_log_sb <- baseball_df %>%
  filter(!is.na(TEAM_BASERUN_SB)) %>%
  filter(TEAM_BASERUN_SB != 0) %>%
  mutate(LOG_BASERUN_SB = log(TEAM_BASERUN_SB))



qqnorm((baseball_log_sb$LOG_BASERUN_SB))
qqline((baseball_log_sb$LOG_BASERUN_SB))
```

The column TEAM_BASERUN_SB is partly correlated with TEAM_BASERUN_CS in the training set. However, there are many misisng values so single imputation may not be an option. In this case, TEAM_BASERUN_SB may qualify for multiple imputation after log transform to make it normally distributed.


```{r, fielding-dp-na}
baseball_df_fix %>%
  filter(is.na(TEAM_FIELDING_DP))

baseball_df_na_dp <- baseball_df_fix %>%
  filter(!is.na(TEAM_FIELDING_DP))

summary(lm(baseball_df_na_dp, formula = TEAM_FIELDING_DP~.-TARGET_WINS))

corrplot(cor(baseball_df_na_dp, use = 'complete.obs'))

hist(baseball_df$TEAM_FIELDING_DP)

qqnorm(baseball_df$TEAM_FIELDING_DP)
qqline(baseball_df$TEAM_FIELDING_DP)

```


## Fitting a Linear Model


My first change to the data was to eliminate the index and, replace HBP and BASERUN_CS with dummy variables.

```{r, baseball-lm}

baseball_df_fix <- baseball_df %>%
  mutate(TEAM_CS_YES_NO = case_when(!is.na(TEAM_BASERUN_CS) ~ 1, is.na(TEAM_BASERUN_CS) ~ 0)) %>% 
  mutate(TEAM_HBP_YES_NO = case_when(!is.na(TEAM_BATTING_HBP) ~ 1, is.na(TEAM_BATTING_HBP) ~ 0)) %>%
  dplyr::select(-c(TEAM_BATTING_HBP, INDEX, TEAM_BASERUN_CS))

baseball_lm <- lm(baseball_df_fix, formula = TARGET_WINS ~.)

summary(baseball_lm)
```

The initial linear model explains 41% of variation. Next, I'll add some log transformations of skewed columns: TEAM_PITCHING_BB, TEAM_PITCHING_SO, TEAM_BASERUN_SB, and TEAM_FIELDING_E.



```{r, baseball-log-lm}


baseball_log_lm <- lm(baseball_df_fix, formula = TARGET_WINS ~.+log(TEAM_FIELDING_E) + log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO) + log(TEAM_BASERUN_SB))

summary(baseball_log_lm)
```

This model explains more variation, but the F-statistic decreased relative to the original model. Next, I'm going to add a few features I'm curious about. TEAM_BATTING_H considers all base hits, including 2B, 3B,and HR. I will create a new variable only looking at singles called TEAM_BATTING_1B. Related to this, I will also incorporate an approximation of an important baseball statistic, slugging. Because some base hits convert to runs at different rates, slugging weighs, singles, doubles, triples and home runs with increasing weight. Usually, slugging also has a denominator of at-bats, which is unavailable in this dataset. Instead, I'll approximate this by dividing by the number of hits. The weights I'm assigning are proportional to the number of bases, so 1 for single, 2 for double... 4 for HR.


```{r, baseball-new-vars-lm}

baseball_df_fix <- baseball_df_fix %>%
  mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR) %>% 
  mutate(TEAM_BATTING_SLG = (TEAM_BATTING_H + TEAM_BATTING_2B + 2 * TEAM_BATTING_3B + 3 *TEAM_BATTING_HR)/TEAM_BATTING_H) ## direct calculation of SLG from TEAM_BATTING_H, which contains 1B + 2B + 3B + Hr


baseball_vars_lm <- lm(baseball_df_fix, formula = TARGET_WINS ~.+log(TEAM_FIELDING_E) + log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO) + log(TEAM_BASERUN_SB))

summary(baseball_vars_lm)
```

Neither of these features offered additional significance. Finally, I'll use back-selection to eliminate non-contributing variables.


```{r, baseball-lm-back-selection}


baseball_back_lm <- lm(baseball_df_fix, formula = TARGET_WINS ~.-TEAM_BATTING_1B+log(TEAM_FIELDING_E) + log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO) + log(TEAM_BASERUN_SB)-TEAM_BATTING_SLG-TEAM_PITCHING_H-TEAM_BATTING_BB-TEAM_BATTING_SO-TEAM_PITCHING_HR-TEAM_PITCHING_BB-TEAM_FIELDING_E)

summary(baseball_back_lm)


```

Before moving to the final model, I want to try creating a simple model with fewer predictors to see how it performs compared to our other models.  To starts I chose a few variables that were highly positively and negatively correlated with TARGET_WINS.  
  * TEAM_BATTING_H
  * TEAM_BATTING_HR
  * TEAM_BATTING_BB
  * TEAM_PITCHING_H
  * TEAM_PITCHING_HR
  * TEAM_PITCHING_BB
  * TEAM_FIELDING_E
  * TEAM_FIELDING_DP
  * TEAM_BATTING_SO
  * TEAM_CS_YES_NO

From there I removed multiple predictors at once.  To do this we need to construct a null hypothesis test which states that removing the variables doesn't make a better model.  We construct a F-test and compare both versions of the model.  If the p-value is under 0.05 we reject the null hypothesis, which indicates our new model isn't different than the first model.  If the p-value is greater than 0.05, the model isn't better with those variables, so I will remove them.  The simpler the model the better.  
  
To determine which variables I removed, I chose the variable that was not proving to be significant in the linear regression (where the p-value was greater than 0.05).  While this doesn't mean the variable itself isn't signficiant, it means the variable alongside the other combination of variables in the model is not significant.    

*Steps:*
  * Remove TEAM_PITCHING_BB
  *
  
  
  * TEAM_PITCHING_BB & TEAM_BATTING_BB
  * TEAM_PITCHING_SO & TEAM_BATTING_SO
  * TEAM_PITCHING_HR & TEAM_BATTING_HR
  * TEAM_PTICHING_H & TEAM_BATTING_H


*Steps:*
  * Remove TEAM_PITCHING_BB & TEAM_PITCHING_SO
```{r model-selection}
m1 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_HR +TEAM_BATTING_BB + TEAM_BATTING_SO  + TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_PITCHING_SO + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_CS_YES_NO,data = baseball_df_fix)
summary(m1)

#remove TEAM_PITCHING_BB & TEAM_PITCHING_SO
m2<- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_HR +TEAM_BATTING_BB + TEAM_BATTING_SO  + TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_CS_YES_NO,data = baseball_df_fix)

summary(m2)
anova(m1, m2)
```

  * Took the log of TEAM_PITCHING_H it's relationship to TARGET_WINS more linear  
```{r forward-selection}
par(mfrow=c(2,1))
plot(baseball_df_fix$TEAM_PITCHING_H,baseball_df_fix$TARGET_WINS,xlab = 'TEAM_PITCHING',ylab = 'TARGET_WINS',main= 'Team Pitching H vs. Target Wins')
plot(log(baseball_df_fix$TEAM_PITCHING_H),baseball_df_fix$TARGET_WINS,xlab = 'LOG(TEAM_PITCHING)',ylab = 'TARGET_WINS')


#log TEAM_PITCHING_H
m3 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_HR +TEAM_BATTING_BB + TEAM_BATTING_SO  + log(TEAM_PITCHING_H) + TEAM_PITCHING_HR + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_CS_YES_NO,data = baseball_df_fix)
summary(m3)
```
  * Remove TEAM_BATTING_H  
```{r model-selection2}
#Remove TEAM_BATTING_H
m4 <- lm(TARGET_WINS ~ TEAM_BATTING_HR +TEAM_BATTING_BB + TEAM_BATTING_SO  + log(TEAM_PITCHING_H) + TEAM_PITCHING_HR + TEAM_FIELDING_E + TEAM_FIELDING_DP + TEAM_CS_YES_NO,data = baseball_df_fix)
summary(m4)
anova(m3, m4)
```

This leaves a model with an R-squared value of ~30, which means the model accounts for 30% of the variance in the data.

## Final Model using all Training Data

For my final model I considered, I originally modeled all of the dummy variables but they ended up not contributing anything to the model. This final model eliminates several features altogether, transforms three, and considers four different interaction effects.

``` {r, baseball-with-interactions}
baseball_interactions <- lm(baseball_df_fix, formula = TARGET_WINS ~ (TEAM_BATTING_H * TEAM_BATTING_2B + TEAM_BATTING_H * TEAM_BATTING_3B + TEAM_BATTING_H * TEAM_BATTING_HR))

summary(baseball_interactions)

```


```{r, final-lm}

baseball_lm2 <- lm(baseball_df_fix, formula = TARGET_WINS ~.-TEAM_BATTING_1B+log(TEAM_FIELDING_E) + log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO)- TEAM_BATTING_SLG-TEAM_PITCHING_H-TEAM_BATTING_BB-TEAM_BATTING_SO-TEAM_PITCHING_HR-TEAM_PITCHING_BB-TEAM_FIELDING_E+log(TEAM_FIELDING_E) +log(TEAM_PITCHING_SO) + TEAM_BATTING_3B:TEAM_BATTING_HR + TEAM_BATTING_2B:TEAM_BATTING_HR +  TEAM_BATTING_H:TEAM_BATTING_HR + TEAM_BATTING_H:TEAM_BATTING_3B- TEAM_BATTING_3B - TEAM_BATTING_SO - TEAM_BATTING_2B-TEAM_BATTING_BB-TEAM_BATTING_HR-TEAM_BATTING_H-TEAM_BATTING_HR- TEAM_PITCHING_HR)


summary(baseball_lm2)


```

## Imputing Missing Values with the Median

```{r}
med <- baseball_df_fix %>% 
   mutate_all(~ifelse(is.na(.), median(., na.rm = TRUE), .))
nonnacols <- baseball_df_fix[complete.cases(baseball_df_fix),]
nonnacols <- nonnacols %>% 
  dplyr::select(INDEX, TEAM_BATTING_SO, TEAM_BASERUN_SB, TEAM_PITCHING_SO, TEAM_FIELDING_DP)

nacols <- baseball_df_fix[!complete.cases(baseball_df_fix),]
nacols <- nacols %>% 
  dplyr::select(INDEX)
nacolsmed <- merge(nacols, med, by = "INDEX")
nacolsmed <- nacolsmed %>% 
  dplyr::select(INDEX, TEAM_BATTING_SO, TEAM_BASERUN_SB, TEAM_PITCHING_SO, TEAM_FIELDING_DP)
evalmed <- regr.eval(nonnacols$TEAM_FIELDING_DP, nacolsmed$TEAM_FIELDING_DP)
evalmed
```
We will transfrom the imputed dataset before running the model
```{r}
med$TEAM_BASERUN_SB  <- log(trainmed$TEAM_BASERUN_SB)
med$TEAM_BATTING_3B <- log(trainmed$TEAM_BATTING_3B)
med$TEAM_PITCHING_BB <- log(trainmed$TEAM_PITCHING_BB)
med$TEAM_PITCHING_SO <- log(trainmed$TEAM_PITCHING_SO)
med$TEAM_FIELDING_E <- log(trainmed$TEAM_FIELDING_E)
```
```{r}
lmmed <- lm(formula = TARGET_WINS ~.-TEAM_BATTING_1B+TEAM_FIELDING_E + TEAM_PITCHING_BB + TEAM_PITCHING_SO- TEAM_BATTING_SLG-TEAM_PITCHING_H-TEAM_BATTING_BB-TEAM_BATTING_SO-TEAM_PITCHING_HR-TEAM_PITCHING_BB- + TEAM_BATTING_3B:TEAM_BATTING_HR + TEAM_BATTING_2B:TEAM_BATTING_HR +  TEAM_BATTING_H:TEAM_BATTING_HR + TEAM_BATTING_H:TEAM_BATTING_3B- TEAM_BATTING_3B - TEAM_BATTING_SO - TEAM_BATTING_2B-TEAM_BATTING_BB-TEAM_BATTING_HR-TEAM_BATTING_H-TEAM_BATTING_HR- TEAM_PITCHING_HR,mbtrainmed)
summary(lmmed)
```

```{r}
res <- resid(lmmed)
plot(density(res))
qqnorm(res)
qqline(res)
```

```{r}

The R-squared statistic indicates that this model predicts less than half of the variation in wins with the included features. For a next step, I hope to use cross-validation techniques to split the training data further and allow me to compare RMSE of various models.


```{r, cv-setup}
#Tom - still working on this
set.seed(123)

cv_index <- createDataPartition(baseball_df_fix$TARGET_WINS, p = 0.8, list = FALSE)

 

train_df <- baseball_df_fix[cv_index,]

test_df <- baseball_df_fix[-cv_index,]

 

cv_control <- trainControl(method = 'cv', number = 5, savePredictions = 'all')



```



### Cross Validation

```{r, cv-performance}


cv_formula <- TARGET_WINS ~ TEAM_BASERUN_SB + TEAM_PITCHING_SO + TEAM_FIELDING_DP + TEAM_CS_YES_NO + TEAM_HBP_YES_NO +  log(TEAM_FIELDING_E) + log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO) + TEAM_BATTING_3B:TEAM_BATTING_HR + TEAM_BATTING_2B:TEAM_BATTING_HR + TEAM_BATTING_3B:TEAM_BATTING_H + TEAM_BATTING_H:TEAM_BATTING_HR


cv_model <- train(form = cv_formula, data = train_df, method = 'lm', trControl = cv_control, na.action = na.pass)

summary(cv_model)

cv_predict <- predict(cv_model, test_df, na.action = na.pass)

cv_resid <- cv_predict-test_df$TARGET_WINS
cv_rmse <- ((mean(cv_resid, na.rm = TRUE))^2)^0.5

cv_rmse


```


As an additional way of measuring model fit, I found a model using backward selection that minimized RMSE.


## Evaluation Data

I also loaded the evaluation data and predicted the wins using my final model. Since the actual wins are withheld, I compared the distribution of predictions to the actual wins in the training set. The means were similar but the training data included much more variation between teams. It's also worth mentioning as well that using the predict function creates missing values as the evaluation data is missing. In fact, for TEAM_BATTING_HBP, over 90% of rows are missing entries.


```{r, eval-data-missing}

round(100*colSums(is.na(baseball_eval))/nrow(baseball_eval),2)


```

The prediction data also has missing values, which are approximately the same as the training data. 


```{r, predict-against-eval-data}

baseball_vars <- baseball_eval %>%
  dplyr::select(TEAM_PITCHING_H, TEAM_PITCHING_HR, TEAM_FIELDING_DP, TEAM_BATTING_3B, TEAM_FIELDING_E, TEAM_PITCHING_BB, TEAM_PITCHING_SO, TEAM_BASERUN_SB, TEAM_BATTING_H, TEAM_BATTING_HR, TEAM_BATTING_2B)

eval_predict <- predict(baseball_interactions, newdata = baseball_eval)




```


```{r, training-vs-eval-wins}

hist(baseball_df$TARGET_WINS)
hist(eval_predict)

summary(eval_predict)
sd(eval_predict)
summary(baseball_df$TARGET_WINS)
sd(baseball_df$TARGET_WINS)
```

```{r, eval-add-new-vars} 

baseball_eval <- baseball_eval %>% ##added new features to eval data so predict could run
  mutate(TEAM_CS_YES_NO = case_when(!is.na(TEAM_BASERUN_CS) ~ 1, is.na(TEAM_BASERUN_CS) ~ 0)) %>% 
  mutate(TEAM_HBP_YES_NO = case_when(!is.na(TEAM_BATTING_HBP) ~ 1, is.na(TEAM_BATTING_HBP) ~ 0))  %>%
  mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR) %>% 
  mutate(TEAM_BATTING_SLG = (TEAM_BATTING_H + TEAM_BATTING_2B + 2 * TEAM_BATTING_3B + 3 *TEAM_BATTING_HR)/TEAM_BATTING_H) ## direct calculation of SLG from TEAM_BATTING_H, which contains 1B + 2B + 3B + Hr


```


```{r, training-vs-eval-wins2}


eval_predict2 <- predict(baseball_lm2, newdata = baseball_eval)

hist(baseball_df$TARGET_WINS, breaks = 40)
hist(eval_predict2)

summary(eval_predict2)
sd(eval_predict2, na.rm = T)

n_test <-nrow(baseball_eval)
n_train <- nrow(baseball_df)

summary(baseball_df$TARGET_WINS)
sd(baseball_df$TARGET_WINS)



```
